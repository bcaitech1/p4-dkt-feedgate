{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nv5EvIVPnz0y"
   },
   "source": [
    "# T-Fixup\n",
    "> 7강 T-Fixup을 베이스라인에 구현해보는 퀴즈의 정답 코드입니다! 아래 베이스라인은 그대로 실행시켜주시고 `T-Fixup` 파트에서 시작해주시면 됩니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 베이스라인\n",
    "> 처음에 주어지는 노트북 베이스라인 코드입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easydict in /opt/conda/lib/python3.7/site-packages (1.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wtJhitPznz06"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import easydict\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import random\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6w3E-ACunz07"
   },
   "source": [
    "### 1. 데이터 로드 및 전처리 컴포넌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "od9O-ttAnz08"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self,args):\n",
    "        self.args = args\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        \n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.train_data\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.test_data\n",
    "\n",
    "    def split_data(self, data, ratio=0.7, shuffle=True, seed=0):\n",
    "        \"\"\"\n",
    "        split data into two parts with a given ratio.\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            random.seed(seed) # fix to default seed 0\n",
    "            random.shuffle(data)\n",
    "\n",
    "        size = int(len(data) * ratio)\n",
    "        data_1 = data[:size]\n",
    "        data_2 = data[size:]\n",
    "\n",
    "        return data_1, data_2\n",
    "\n",
    "    def __save_labels(self, encoder, name):\n",
    "        le_path = os.path.join(self.args.asset_dir, name + '_classes.npy')\n",
    "        np.save(le_path, encoder.classes_)\n",
    "\n",
    "    def __preprocessing(self, df, is_train = True):\n",
    "        cate_cols = ['assessmentItemID', 'testId', 'KnowledgeTag']\n",
    "\n",
    "        if not os.path.exists(self.args.asset_dir):\n",
    "            os.makedirs(self.args.asset_dir)\n",
    "            \n",
    "        for col in cate_cols:\n",
    "            \n",
    "            \n",
    "            le = LabelEncoder()\n",
    "            if is_train:\n",
    "                #For UNKNOWN class\n",
    "                a = df[col].unique().tolist() + ['unknown']\n",
    "                le.fit(a)\n",
    "                self.__save_labels(le, col)\n",
    "            else:\n",
    "                label_path = os.path.join(self.args.asset_dir,col+'_classes.npy')\n",
    "                le.classes_ = np.load(label_path)\n",
    "                \n",
    "                df[col] = df[col].apply(lambda x: x if x in le.classes_ else 'unknown')\n",
    "\n",
    "            #모든 컬럼이 범주형이라고 가정\n",
    "            df[col]= df[col].astype(str)\n",
    "            test = le.transform(df[col])\n",
    "            df[col] = test\n",
    "            \n",
    "\n",
    "        def convert_time(s):\n",
    "            timestamp = time.mktime(datetime.strptime(s, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "            return int(timestamp)\n",
    "\n",
    "        df['Timestamp'] = df['Timestamp'].apply(convert_time)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __feature_engineering(self, df):\n",
    "        #TODO\n",
    "        return df\n",
    "\n",
    "    def load_data_from_file(self, file_name, is_train=True):\n",
    "        csv_file_path = os.path.join(self.args.data_dir, file_name)\n",
    "        df = pd.read_csv(csv_file_path)#, nrows=100000)\n",
    "        df = self.__feature_engineering(df)\n",
    "        df = self.__preprocessing(df, is_train)\n",
    "\n",
    "        # 추후 feature를 embedding할 시에 embedding_layer의 input 크기를 결정할때 사용\n",
    "\n",
    "                \n",
    "        self.args.n_questions = len(np.load(os.path.join(self.args.asset_dir,'assessmentItemID_classes.npy')))\n",
    "        self.args.n_test = len(np.load(os.path.join(self.args.asset_dir,'testId_classes.npy')))\n",
    "        self.args.n_tag = len(np.load(os.path.join(self.args.asset_dir,'KnowledgeTag_classes.npy')))\n",
    "        \n",
    "\n",
    "\n",
    "        df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "        columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag']\n",
    "        group = df[columns].groupby('userID').apply(\n",
    "                lambda r: (\n",
    "                    r['testId'].values, \n",
    "                    r['assessmentItemID'].values,\n",
    "                    r['KnowledgeTag'].values,\n",
    "                    r['answerCode'].values\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return group.values\n",
    "\n",
    "    def load_train_data(self, file_name):\n",
    "        self.train_data = self.load_data_from_file(file_name)\n",
    "\n",
    "    def load_test_data(self, file_name):\n",
    "        self.test_data = self.load_data_from_file(file_name, is_train= False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-MQhPevnz08"
   },
   "source": [
    "### 2. 데이터 셋 / 데이터 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h29rn8YNnz09"
   },
   "outputs": [],
   "source": [
    "class DKTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "\n",
    "        # 각 data의 sequence length\n",
    "        seq_len = len(row[0])\n",
    "\n",
    "        test, question, tag, correct = row[0], row[1], row[2], row[3]\n",
    "        \n",
    "\n",
    "        cate_cols = [test, question, tag, correct]\n",
    "\n",
    "        # max seq len을 고려하여서 이보다 길면 자르고 아닐 경우 그대로 냅둔다\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cate_cols):\n",
    "                cate_cols[i] = col[-self.args.max_seq_len:]\n",
    "            mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "        else:\n",
    "            mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "            mask[-seq_len:] = 1\n",
    "\n",
    "        # mask도 columns 목록에 포함시킴\n",
    "        cate_cols.append(mask)\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cate_cols):\n",
    "            cate_cols[i] = torch.tensor(col)\n",
    "\n",
    "        return cate_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    col_n = len(batch[0])\n",
    "    col_list = [[] for _ in range(col_n)]\n",
    "    max_seq_len = len(batch[0][-1])\n",
    "\n",
    "        \n",
    "    # batch의 값들을 각 column끼리 그룹화\n",
    "    for row in batch:\n",
    "        for i, col in enumerate(row):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col\n",
    "            col_list[i].append(pre_padded)\n",
    "\n",
    "\n",
    "    for i, _ in enumerate(col_list):\n",
    "        col_list[i] =torch.stack(col_list[i])\n",
    "    \n",
    "    return tuple(col_list)\n",
    "\n",
    "\n",
    "def get_loaders(args, train, valid):\n",
    "\n",
    "    pin_memory = False\n",
    "    train_loader, valid_loader = None, None\n",
    "    \n",
    "    if train is not None:\n",
    "        trainset = DKTDataset(train, args)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, num_workers=args.num_workers, shuffle=True,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "    if valid is not None:\n",
    "        valset = DKTDataset(valid, args)\n",
    "        valid_loader = torch.utils.data.DataLoader(valset, num_workers=args.num_workers, shuffle=False,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyiplxY6nz0-"
   },
   "source": [
    "### 3. BERT 기반의 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aO72oKAgnz0-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "try:\n",
    "    from transformers.modeling_bert import BertConfig, BertEncoder, BertModel    \n",
    "except:\n",
    "    from transformers.models.bert.modeling_bert import BertConfig, BertEncoder, BertModel    \n",
    "class Bert(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(Bert, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct으로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "\n",
    "        \n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim)\n",
    "\n",
    "        # Bert config\n",
    "        self.config = BertConfig( \n",
    "            3, # not used\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_hidden_layers=self.args.n_layers,\n",
    "            num_attention_heads=self.args.n_heads,\n",
    "            max_position_embeddings=self.args.max_seq_len          \n",
    "        )\n",
    "\n",
    "        # Defining the layers\n",
    "        # Bert Layer\n",
    "        self.encoder = BertModel(self.config)  \n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.args.hidden_dim, 1)\n",
    "       \n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        test, question, tag, _, mask, interaction, _ = input\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # 신나는 embedding\n",
    "        \n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        \n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        \n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "        \n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "        \n",
    "                           embed_tag,], 2)\n",
    "\n",
    "        X = self.comb_proj(embed)\n",
    "\n",
    "        # Bert\n",
    "        encoded_layers = self.encoder(inputs_embeds=X, attention_mask=mask)\n",
    "        out = encoded_layers[0]\n",
    "\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEaAa6Prnz0_"
   },
   "source": [
    "### 4. 모델 훈련을 위한 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r_wU37QGnz0_"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "# 훈련을 하기 위한 세팅\n",
    "def get_optimizer(model, args):\n",
    "    if args.optimizer == 'adam':\n",
    "        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
    "    if args.optimizer == 'adamW':\n",
    "        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
    "    \n",
    "    # 모든 parameter들의 grad값을 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def get_scheduler(optimizer, args):\n",
    "    if args.scheduler == 'plateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.5, mode='max', verbose=True)\n",
    "    elif args.scheduler == 'linear_warmup':\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=args.warmup_steps,\n",
    "                                                    num_training_steps=args.total_steps)\n",
    "    return scheduler\n",
    "\n",
    "def get_criterion(pred, target):\n",
    "    loss = nn.BCELoss(reduction=\"none\")\n",
    "    return loss(pred, target)\n",
    "\n",
    "def get_metric(targets, preds):\n",
    "    auc = roc_auc_score(targets, preds)\n",
    "    acc = accuracy_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "\n",
    "    return auc, acc\n",
    "\n",
    "def get_model(args):\n",
    "    \"\"\"\n",
    "    Load model and move tensors to a given devices.\n",
    "    \"\"\"\n",
    "    if args.model == 'bert': model = Bert(args)\n",
    "    \n",
    "\n",
    "    model.to(args.device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# 배치 전처리\n",
    "def process_batch(batch, args):\n",
    "\n",
    "    test, question, tag, correct, mask = batch\n",
    "    \n",
    "    \n",
    "    # change to float\n",
    "    mask = mask.type(torch.FloatTensor)\n",
    "    correct = correct.type(torch.FloatTensor)\n",
    "\n",
    "    #  interaction을 임시적으로 correct를 한칸 우측으로 이동한 것으로 사용\n",
    "    #    saint의 경우 decoder에 들어가는 input이다\n",
    "    interaction = correct + 1 # 패딩을 위해 correct값에 1을 더해준다.\n",
    "    interaction = interaction.roll(shifts=1, dims=1)\n",
    "    interaction[:, 0] = 0 # set padding index to the first sequence\n",
    "    interaction = (interaction * mask).to(torch.int64)\n",
    "    # print(interaction)\n",
    "    # exit()\n",
    "    #  test_id, question_id, tag\n",
    "    test = ((test + 1) * mask).to(torch.int64)\n",
    "    question = ((question + 1) * mask).to(torch.int64)\n",
    "    tag = ((tag + 1) * mask).to(torch.int64)\n",
    "\n",
    "    # gather index\n",
    "    # 마지막 sequence만 사용하기 위한 index\n",
    "    gather_index = torch.tensor(np.count_nonzero(mask, axis=1))\n",
    "    gather_index = gather_index.view(-1, 1) - 1\n",
    "\n",
    "\n",
    "    # device memory로 이동\n",
    "\n",
    "    test = test.to(args.device)\n",
    "    question = question.to(args.device)\n",
    "\n",
    "\n",
    "    tag = tag.to(args.device)\n",
    "    correct = correct.to(args.device)\n",
    "    mask = mask.to(args.device)\n",
    "\n",
    "    interaction = interaction.to(args.device)\n",
    "    gather_index = gather_index.to(args.device)\n",
    "\n",
    "    return (test, question,\n",
    "            tag, correct, mask,\n",
    "            interaction, gather_index)\n",
    "\n",
    "\n",
    "# loss계산하고 parameter update!\n",
    "def compute_loss(preds, targets):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        preds   : (batch_size, max_seq_len)\n",
    "        targets : (batch_size, max_seq_len)\n",
    "\n",
    "    \"\"\"\n",
    "    loss = get_criterion(preds, targets)\n",
    "    #마지막 시퀀드에 대한 값만 loss 계산\n",
    "    loss = loss[:,-1]\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def update_params(loss, model, optimizer, args):\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, model_dir, model_filename):\n",
    "    print('saving model ...')\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)    \n",
    "    torch.save(state, os.path.join(model_dir, model_filename))\n",
    "\n",
    "\n",
    "\n",
    "def load_model(args):\n",
    "    \n",
    "    \n",
    "    model_path = os.path.join(args.model_dir, args.model_name)\n",
    "    print(\"Loading Model from:\", model_path)\n",
    "    load_state = torch.load(model_path)\n",
    "    model = get_model(args)\n",
    "\n",
    "    # 1. load model state\n",
    "    model.load_state_dict(load_state['state_dict'], strict=True)\n",
    "   \n",
    "    \n",
    "    print(\"Loading Model from:\", model_path, \"...Finished.\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YO_xFaJYnz1B"
   },
   "source": [
    "### 5. 전체 프로세스를 담당하는 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BMiIOHgJnz1D"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run(args, train_data, valid_data):\n",
    "    train_loader, valid_loader = get_loaders(args, train_data, valid_data)\n",
    "    \n",
    "    # only when using warmup scheduler\n",
    "    args.total_steps = int(len(train_loader.dataset) / args.batch_size) * (args.n_epochs)\n",
    "    args.warmup_steps = args.total_steps // 10\n",
    "            \n",
    "    model = get_model(args)\n",
    "    optimizer = get_optimizer(model, args)\n",
    "    scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "    best_auc = -1\n",
    "    early_stopping_counter = 0\n",
    "    for epoch in range(args.n_epochs):\n",
    "\n",
    "        print(f\"Start Training: Epoch {epoch + 1}\")\n",
    "        \n",
    "        ### TRAIN\n",
    "        train_auc, train_acc, train_loss = train(train_loader, model, optimizer, args)\n",
    "        \n",
    "        ### VALID\n",
    "        auc, acc, _, _ = validate(valid_loader, model, args)\n",
    "\n",
    "        ### TODO: model save or early stopping\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_auc\": train_auc, \"train_acc\":train_acc,\n",
    "                  \"valid_auc\":auc, \"valid_acc\":acc})\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model_to_save.state_dict(),\n",
    "                },\n",
    "                args.model_dir, 'model.pt',\n",
    "            )\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= args.patience:\n",
    "                print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n",
    "                break\n",
    "\n",
    "        # scheduler\n",
    "        if args.scheduler == 'plateau':\n",
    "            scheduler.step(best_auc)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "def train(train_loader, model, optimizer, args):\n",
    "    model.train()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    losses = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input = process_batch(batch, args)\n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "\n",
    "\n",
    "        loss = compute_loss(preds, targets)\n",
    "        update_params(loss, model, optimizer, args)\n",
    "\n",
    "        if step % args.log_steps == 0:\n",
    "            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n",
    "        \n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "        \n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "        losses.append(loss)\n",
    "      \n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    loss_avg = sum(losses)/len(losses)\n",
    "    print(f'TRAIN AUC : {auc} ACC : {acc}')\n",
    "    return auc, acc, loss_avg\n",
    "    \n",
    "\n",
    "def validate(valid_loader, model, args):\n",
    "    model.eval()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "    \n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    \n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "\n",
    "    return auc, acc, total_preds, total_targets\n",
    "\n",
    "\n",
    "\n",
    "def inference(args, test_data):\n",
    "    \n",
    "    model = load_model(args)\n",
    "    model.eval()\n",
    "    _, test_loader = get_loaders(args, None, test_data)\n",
    "    \n",
    "    \n",
    "    total_preds = []\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        \n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            \n",
    "        total_preds+=list(preds)\n",
    "\n",
    "    write_path = os.path.join(args.output_dir, \"output.csv\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)    \n",
    "    with open(write_path, 'w', encoding='utf8') as w:\n",
    "        print(\"writing prediction : {}\".format(write_path))\n",
    "        w.write(\"id,prediction\\n\")\n",
    "        for id, p in enumerate(total_preds):\n",
    "            w.write('{},{}\\n'.format(id,p))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPEE00qUnz1E"
   },
   "source": [
    "### 6.실행부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qZmwQenqnz1E"
   },
   "outputs": [],
   "source": [
    "data_dir = '/opt/ml/input/data/train_dataset'\n",
    "file_name = 'train_data.csv'\n",
    "test_file_name = 'test_data.csv'\n",
    "\n",
    "config = {}\n",
    "\n",
    "# 설정\n",
    "config['seed'] = 42\n",
    "config['device'] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config['data_dir'] = data_dir\n",
    "config['asset_dir'] = 'asset'\n",
    "config['model_dir'] = 'models'\n",
    "config['model_name'] = 'model.pt'\n",
    "config['output_dir'] = 'output'\n",
    "\n",
    "# 데이터\n",
    "config['max_seq_len'] = 20\n",
    "config['num_workers'] = 1\n",
    "\n",
    "\n",
    "# 모델\n",
    "config['hidden_dim'] = 64\n",
    "config['n_layers'] = 2\n",
    "config['n_heads'] = 4\n",
    "config['dropout'] = 0.2\n",
    "\n",
    "# 훈련\n",
    "config['n_epochs'] = 20\n",
    "config['batch_size'] = 64\n",
    "config['lr'] = 0.0001\n",
    "config['clip_grad'] = 10\n",
    "config['log_steps'] = 50\n",
    "config['patience'] = 5\n",
    "\n",
    "\n",
    "\n",
    "### 중요 ###\n",
    "config['model'] = 'bert'\n",
    "config['optimizer'] = 'adam'\n",
    "config['scheduler'] = 'plateau'\n",
    "\n",
    "\n",
    "args = easydict.EasyDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setSeeds(seed = 42):\n",
    "    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)    \n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rNaRoFrLnz1E"
   },
   "outputs": [],
   "source": [
    "setSeeds(42)\n",
    "\n",
    "preprocess = Preprocess(args)\n",
    "preprocess.load_train_data(file_name)\n",
    "\n",
    "train_data = preprocess.get_train_data()\n",
    "train_data, valid_data = preprocess.split_data(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Fixup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Fixup 코드\n",
    "> 조금 더 자세한 설명은 7강 가중치 초기화 (Weight Initialization)을 참조하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class YourModel:\n",
    "    def __init__(self):\n",
    "        # T-Fixup\n",
    "        if self.args.Tfixup:\n",
    "\n",
    "            # 초기화 (Initialization)\n",
    "            self.tfixup_initialization()\n",
    "            print(\"T-Fixup Initialization Done\")\n",
    "\n",
    "            # 스케일링 (Scaling)\n",
    "            self.tfixup_scaling()\n",
    "            print(f\"T-Fixup Scaling Done\")\n",
    "\n",
    "    def tfixup_initialization(self):\n",
    "        # 우리는 padding idx의 경우 모두 0으로 통일한다\n",
    "        padding_idx = 0\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if re.match(r'^embedding*', name):\n",
    "                nn.init.normal_(param, mean=0, std=param.shape[1] ** -0.5)\n",
    "                nn.init.constant_(param[padding_idx], 0)\n",
    "            elif re.match(r'.*Norm.*', name):\n",
    "                continue\n",
    "            elif re.match(r'.*weight*', name):\n",
    "                # nn.init.xavier_uniform_(param)\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "\n",
    "    def tfixup_scaling(self):\n",
    "        temp_state_dict = {}\n",
    "\n",
    "        # 특정 layer들의 값을 스케일링한다\n",
    "        for name, param in self.named_parameters():\n",
    "\n",
    "            # TODO: 모델 내부의 module 이름이 달라지면 직접 수정해서\n",
    "            #       module이 scaling 될 수 있도록 변경해주자\n",
    "            # print(name)\n",
    "\n",
    "            if re.match(r'^embedding*', name):\n",
    "                temp_state_dict[name] = (9 * self.args.n_layers) ** (-1 / 4) * param   \n",
    "            elif re.match(r'.*Norm.*', name):\n",
    "                continue\n",
    "            elif re.match(r'encoder.*dense.*weight$|encoder.*attention.output.*weight$', name):\n",
    "                temp_state_dict[name] = (0.67 * (self.args.n_layers) ** (-1 / 4)) * param\n",
    "            elif re.match(r\"encoder.*value.weight$\", name):\n",
    "                temp_state_dict[name] = (0.67 * (self.args.n_layers) ** (-1 / 4)) * (param * (2**0.5))\n",
    "\n",
    "        # 나머지 layer는 원래 값 그대로 넣는다\n",
    "        for name in self.state_dict():\n",
    "            if name not in temp_state_dict:\n",
    "                temp_state_dict[name] = self.state_dict()[name]\n",
    "                \n",
    "        self.load_state_dict(temp_state_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Fixup Bert 베이스라인 모델에 적용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfixupBert(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(TfixupBert, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct으로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim)\n",
    "\n",
    "        # Bert config\n",
    "        self.config = BertConfig( \n",
    "            3, # not used\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_hidden_layers=self.args.n_layers,\n",
    "            num_attention_heads=self.args.n_heads,\n",
    "            max_position_embeddings=self.args.max_seq_len          \n",
    "        )\n",
    "\n",
    "        # Defining the layers\n",
    "        # Bert Layer\n",
    "        self.encoder = BertModel(self.config)  \n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.args.hidden_dim, 1)\n",
    "       \n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "        # T-Fixup\n",
    "        if self.args.Tfixup:\n",
    "\n",
    "            # 초기화 (Initialization)\n",
    "            self.tfixup_initialization()\n",
    "            print(\"T-Fixupbb Initialization Done\")\n",
    "\n",
    "            # 스케일링 (Scaling)\n",
    "            self.tfixup_scaling()\n",
    "            print(f\"T-Fixup Scaling Done\")\n",
    "\n",
    "    def tfixup_initialization(self):\n",
    "        # 우리는 padding idx의 경우 모두 0으로 통일한다\n",
    "        padding_idx = 0\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if re.match(r'^embedding*', name):\n",
    "                nn.init.normal_(param, mean=0, std=param.shape[1] ** -0.5)\n",
    "                nn.init.constant_(param[padding_idx], 0)\n",
    "            elif re.match(r'.*Norm.*', name):\n",
    "                continue\n",
    "            elif re.match(r'.*weight*', name):\n",
    "                # nn.init.xavier_uniform_(param)\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "\n",
    "    def tfixup_scaling(self):\n",
    "        temp_state_dict = {}\n",
    "\n",
    "        # 특정 layer들의 값을 스케일링한다\n",
    "        for name, param in self.named_parameters():\n",
    "\n",
    "            # TODO: 모델 내부의 module 이름이 달라지면 직접 수정해서\n",
    "            #       module이 scaling 될 수 있도록 변경해주자\n",
    "            # print(name)\n",
    "\n",
    "            if re.match(r'^embedding*', name):\n",
    "                temp_state_dict[name] = (9 * self.args.n_layers) ** (-1 / 4) * param   \n",
    "            elif re.match(r'.*Norm.*', name):\n",
    "                continue\n",
    "            elif re.match(r'encoder.*dense.*weight$|encoder.*attention.output.*weight$', name):\n",
    "                temp_state_dict[name] = (0.67 * (self.args.n_layers) ** (-1 / 4)) * param\n",
    "            elif re.match(r\"encoder.*value.weight$\", name):\n",
    "                temp_state_dict[name] = (0.67 * (self.args.n_layers) ** (-1 / 4)) * (param * (2**0.5))\n",
    "\n",
    "        # 나머지 layer는 원래 값 그대로 넣는다\n",
    "        for name in self.state_dict():\n",
    "            if name not in temp_state_dict:\n",
    "                temp_state_dict[name] = self.state_dict()[name]\n",
    "                \n",
    "        self.load_state_dict(temp_state_dict)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        test, question, tag, _, mask, interaction, _ = input\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # 신나는 embedding\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_tag,], 2)\n",
    "\n",
    "        X = self.comb_proj(embed)\n",
    "\n",
    "        # Bert\n",
    "        encoded_layers = self.encoder(inputs_embeds=X, attention_mask=mask)\n",
    "        out = encoded_layers[0]\n",
    "\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    \"\"\"\n",
    "    Load model and move tensors to a given devices.\n",
    "    \"\"\"\n",
    "    if args.model == 'bert': model = Bert(args)\n",
    "    if args.model == 'tfixup_bert': model = TfixupBert(args)\n",
    "\n",
    "    model.to(args.device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert 훈련시키기 (20 Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbomichoi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dandy-voice-21</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/bomichoi/dkt\" target=\"_blank\">https://wandb.ai/bomichoi/dkt</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/bomichoi/dkt/runs/3gbesqw7\" target=\"_blank\">https://wandb.ai/bomichoi/dkt/runs/3gbesqw7</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/p4-dkt-feedgate/wandb/run-20210602_040704-3gbesqw7</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(3gbesqw7)</h1><iframe src=\"https://wandb.ai/bomichoi/dkt/runs/3gbesqw7\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7faec15c6c90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='dkt', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.7601020336151123\n",
      "Training steps: 50 Loss: 0.6398704648017883\n",
      "TRAIN AUC : 0.6701844367845223 ACC : 0.638438566552901\n",
      "VALID AUC : 0.7082144405069719 ACC : 0.6587064676616915\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.5946140289306641\n",
      "Training steps: 50 Loss: 0.6746055483818054\n",
      "TRAIN AUC : 0.711125920428988 ACC : 0.6565699658703071\n",
      "VALID AUC : 0.7112642460632713 ACC : 0.6626865671641791\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.6111980676651001\n",
      "Training steps: 50 Loss: 0.6411390900611877\n",
      "TRAIN AUC : 0.7149466636642852 ACC : 0.6604095563139932\n",
      "VALID AUC : 0.7030173734777008 ACC : 0.6358208955223881\n",
      "\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.6101197004318237\n",
      "Training steps: 50 Loss: 0.5977492928504944\n",
      "TRAIN AUC : 0.7167941311341801 ACC : 0.6599829351535836\n",
      "VALID AUC : 0.7139570012129796 ACC : 0.6611940298507463\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.5861613750457764\n",
      "Training steps: 50 Loss: 0.640864372253418\n",
      "TRAIN AUC : 0.7228990242563864 ACC : 0.6668088737201365\n",
      "VALID AUC : 0.7128342760455874 ACC : 0.6646766169154229\n",
      "\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.5497426390647888\n",
      "Training steps: 50 Loss: 0.6077958941459656\n",
      "TRAIN AUC : 0.7231727028818304 ACC : 0.669155290102389\n",
      "VALID AUC : 0.7157392778258804 ACC : 0.663681592039801\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.7493699789047241\n",
      "Training steps: 50 Loss: 0.6161885261535645\n",
      "TRAIN AUC : 0.7239955620667933 ACC : 0.6680887372013652\n",
      "VALID AUC : 0.7155617444292698 ACC : 0.66318407960199\n",
      "\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.5036529898643494\n",
      "Training steps: 50 Loss: 0.6160316467285156\n",
      "TRAIN AUC : 0.7269055626411356 ACC : 0.6685153583617748\n",
      "VALID AUC : 0.7173628653691851 ACC : 0.6696517412935323\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.6258493661880493\n",
      "Training steps: 50 Loss: 0.6863818168640137\n",
      "TRAIN AUC : 0.7246650809959276 ACC : 0.672141638225256\n",
      "VALID AUC : 0.7158285404275282 ACC : 0.6676616915422886\n",
      "\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.6495794653892517\n",
      "Training steps: 50 Loss: 0.6258941888809204\n",
      "TRAIN AUC : 0.726657592667382 ACC : 0.6668088737201365\n",
      "VALID AUC : 0.7153762765791793 ACC : 0.6666666666666666\n",
      "\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.5689733028411865\n",
      "Training steps: 50 Loss: 0.6407856345176697\n",
      "TRAIN AUC : 0.7298509354029101 ACC : 0.6764078498293515\n",
      "VALID AUC : 0.7164980099398865 ACC : 0.6621890547263681\n",
      "\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.6041098237037659\n",
      "Training steps: 50 Loss: 0.6072666049003601\n",
      "TRAIN AUC : 0.7298423658523466 ACC : 0.6749146757679181\n",
      "VALID AUC : 0.7165396658206555 ACC : 0.6606965174129353\n",
      "\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.5419753789901733\n",
      "Training steps: 50 Loss: 0.5854064226150513\n",
      "TRAIN AUC : 0.7318314132374031 ACC : 0.6695819112627986\n",
      "VALID AUC : 0.7167965437520643 ACC : 0.6681592039800995\n",
      "\n",
      "EarlyStopping counter: 5 out of 5\n"
     ]
    }
   ],
   "source": [
    "args.model = 'bert'\n",
    "args.n_layers = 20\n",
    "\n",
    "run(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Fixup Bert 훈련시키기 (20 Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Fixupbb Initialization Done\n",
      "T-Fixup Scaling Done\n",
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.9224938154220581\n",
      "Training steps: 50 Loss: 0.6830382347106934\n",
      "TRAIN AUC : 0.5391839052900566 ACC : 0.5305034129692833\n",
      "VALID AUC : 0.686463128098776 ACC : 0.5208955223880597\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.6878604292869568\n",
      "Training steps: 50 Loss: 0.6104351878166199\n",
      "TRAIN AUC : 0.6254563057761505 ACC : 0.5910836177474402\n",
      "VALID AUC : 0.7150985707073864 ACC : 0.5323383084577115\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.7068123817443848\n",
      "Training steps: 50 Loss: 0.7003744840621948\n",
      "TRAIN AUC : 0.683059183686493 ACC : 0.6348122866894198\n",
      "VALID AUC : 0.7241597165813217 ACC : 0.6537313432835821\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.7406575083732605\n",
      "Training steps: 50 Loss: 0.6597524285316467\n",
      "TRAIN AUC : 0.7071262194516035 ACC : 0.6569965870307167\n",
      "VALID AUC : 0.7277401387140829 ACC : 0.6751243781094527\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.5054775476455688\n",
      "Training steps: 50 Loss: 0.600224494934082\n",
      "TRAIN AUC : 0.7204938978418408 ACC : 0.6663822525597269\n",
      "VALID AUC : 0.7317743124052205 ACC : 0.6577114427860696\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.5944494009017944\n",
      "Training steps: 50 Loss: 0.5332187414169312\n",
      "TRAIN AUC : 0.7325621953365234 ACC : 0.6789675767918089\n",
      "VALID AUC : 0.734916851886565 ACC : 0.6840796019900498\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.6057265996932983\n",
      "Training steps: 50 Loss: 0.6326121091842651\n",
      "TRAIN AUC : 0.7280923542287541 ACC : 0.6749146757679181\n",
      "VALID AUC : 0.7384149540644733 ACC : 0.6850746268656717\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.5804967284202576\n",
      "Training steps: 50 Loss: 0.6459673047065735\n",
      "TRAIN AUC : 0.7483305330351615 ACC : 0.6860068259385665\n",
      "VALID AUC : 0.7417821377599648 ACC : 0.6850746268656717\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.5016001462936401\n",
      "Training steps: 50 Loss: 0.5300295352935791\n",
      "TRAIN AUC : 0.765753887977741 ACC : 0.6975255972696246\n",
      "VALID AUC : 0.7452445348972141 ACC : 0.6940298507462687\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.5871093273162842\n",
      "Training steps: 50 Loss: 0.6833832263946533\n",
      "TRAIN AUC : 0.7775574410592694 ACC : 0.70669795221843\n",
      "VALID AUC : 0.7487624732088218 ACC : 0.6860696517412935\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.5838922262191772\n",
      "Training steps: 50 Loss: 0.615671694278717\n",
      "TRAIN AUC : 0.7750522150009072 ACC : 0.7073378839590444\n",
      "VALID AUC : 0.7505645859554223 ACC : 0.6935323383084577\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.5048792362213135\n",
      "Training steps: 50 Loss: 0.43676382303237915\n",
      "TRAIN AUC : 0.797033294527248 ACC : 0.7222696245733788\n",
      "VALID AUC : 0.7499090017366534 ACC : 0.681592039800995\n",
      "\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.5006664991378784\n",
      "Training steps: 50 Loss: 0.5343942642211914\n",
      "TRAIN AUC : 0.7968723875406027 ACC : 0.7305887372013652\n",
      "VALID AUC : 0.750534831754873 ACC : 0.6870646766169154\n",
      "\n",
      "Start Training: Epoch 14\n",
      "Training steps: 0 Loss: 0.5113525390625\n",
      "Training steps: 50 Loss: 0.5191720128059387\n",
      "TRAIN AUC : 0.8209737015079674 ACC : 0.7478668941979523\n",
      "VALID AUC : 0.7482566517994844 ACC : 0.6835820895522388\n",
      "\n",
      "Start Training: Epoch 15\n",
      "Training steps: 0 Loss: 0.486730694770813\n",
      "Training steps: 50 Loss: 0.7189884185791016\n",
      "TRAIN AUC : 0.8255035294696816 ACC : 0.7497866894197952\n",
      "VALID AUC : 0.7472509598209194 ACC : 0.6756218905472637\n",
      "\n",
      "Start Training: Epoch 16\n",
      "Training steps: 0 Loss: 0.5409607291221619\n",
      "Training steps: 50 Loss: 0.5685566663742065\n",
      "TRAIN AUC : 0.8295233780074336 ACC : 0.7534129692832765\n",
      "VALID AUC : 0.7451860183028006 ACC : 0.6850746268656717\n",
      "\n",
      "EarlyStopping counter: 5 out of 5\n"
     ]
    }
   ],
   "source": [
    "args.model = 'tfixup_bert'\n",
    "args.Tfixup = True\n",
    "args.epoch = 30\n",
    "args.n_layers = 20\n",
    "\n",
    "run(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder에 T-Fixup 적용해보기\n",
    "> Hugging Face의 Bert를 사용하면 `Layer Norm`을 사용하지 않기가 어렵다! 직접 Bert 내부를 구현하면서 모델을 좀 더 custom하게 다뤄보자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Fixup 모델 생성\n",
    "- Encoder Layer\n",
    "- Fixup Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ☘️ Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "\n",
    "        self.query = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "        self.key = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "        self.value = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=self.hidden_dim, num_heads=self.args.n_heads)\n",
    "\n",
    "        self.ffn1 = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "        self.ffn2 = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)   \n",
    "\n",
    "        if self.args.layer_norm:\n",
    "            self.ln1 = nn.LayerNorm(self.hidden_dim)\n",
    "            self.ln2 = nn.LayerNorm(self.hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, embed, mask):\n",
    "        q = self.query(embed).permute(1, 0, 2)\n",
    "        k = self.key(embed).permute(1, 0, 2)\n",
    "        v = self.value(embed).permute(1, 0, 2)\n",
    "\n",
    "        ## attention\n",
    "        out, _ = self.attn(q, k, v, attn_mask=mask)\n",
    "        \n",
    "        ## residual + layer norm\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = embed + out\n",
    "        \n",
    "        if self.args.layer_norm:\n",
    "            out = self.ln1(out)\n",
    "\n",
    "        ## feed forward network\n",
    "        out = self.ffn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.ffn2(out)\n",
    "\n",
    "        ## residual + layer norm\n",
    "        out = embed + out\n",
    "\n",
    "        if self.args.layer_norm:\n",
    "            out = self.ln2(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ☘️ Fixup Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FixupEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(FixupEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct으로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "        self.embedding_position = nn.Embedding(self.args.max_seq_len, self.hidden_dim)\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoders = nn.ModuleList([EncoderLayer(args) for _ in range(self.n_layers)])\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.args.hidden_dim, 1)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        # T-Fixup\n",
    "        if self.args.Tfixup:\n",
    "\n",
    "            # 초기화 (Initialization)\n",
    "            self.tfixup_initialization()\n",
    "            print(\"T-Fixup Initialization Done\")\n",
    "\n",
    "            # 스케일링 (Scaling)\n",
    "            self.tfixup_scaling()\n",
    "            print(f\"T-Fixup Scaling Done\")\n",
    "\n",
    "    def tfixup_initialization(self):\n",
    "        # 우리는 padding idx의 경우 모두 0으로 통일한다\n",
    "        padding_idx = 0\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if re.match(r'^embedding*', name):\n",
    "                nn.init.normal_(param, mean=0, std=param.shape[1] ** -0.5)\n",
    "                nn.init.constant_(param[padding_idx], 0)\n",
    "            elif re.match(r'.*ln.*|.*bn.*', name):\n",
    "                continue\n",
    "            elif re.match(r'.*weight*', name):\n",
    "                # scaling없이 아래 가중치 초기화만 사용하면\n",
    "                # AUC가 0.5로 성능이 심각하게 저하된다\n",
    "                # nn.init.xavier_uniform_(param)\n",
    "                nn.init.xavier_normal_(param)\n",
    "                # nn.init.kaiming_uniform_(param)\n",
    "                # nn.init.kaiming_normal_(param)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def tfixup_scaling(self):\n",
    "        temp_state_dict = {}\n",
    "\n",
    "        # 특정 layer들의 값을 스케일링한다\n",
    "        for name, param in self.named_parameters():\n",
    "\n",
    "            # TODO: 모델 내부의 module 이름이 달라지면 직접 수정해서\n",
    "            #       module이 scaling 될 수 있도록 변경해주자\n",
    "            # print(name)\n",
    "\n",
    "            if re.match(r'^embedding*', name):\n",
    "                temp_state_dict[name] = (9 * self.args.n_layers) ** (-1 / 4) * param          \n",
    "            elif re.match(r'encoder.*ffn.*weight$|.*attn.out_proj.weight$', name):\n",
    "                temp_state_dict[name] = (0.67 * (self.args.n_layers) ** (-1 / 4)) * param\n",
    "            elif re.match(r\".*value.weight$\", name):\n",
    "                temp_state_dict[name] = (0.67 * (self.args.n_layers) ** (-1 / 4)) * (param * (2**0.5))\n",
    "\n",
    "        # 나머지 layer는 원래 값 그대로 넣는다\n",
    "        for name in self.state_dict():\n",
    "            if name not in temp_state_dict:\n",
    "                temp_state_dict[name] = self.state_dict()[name]\n",
    "\n",
    "        self.load_state_dict(temp_state_dict)\n",
    "\n",
    "    def mask_2d_to_3d(self, mask, batch_size, seq_len):\n",
    "        # padding 부분에 1을 주기 위해 0과 1을 뒤집는다\n",
    "        mask = torch.ones_like(mask) - mask\n",
    "        \n",
    "        mask = mask.repeat(1, seq_len)\n",
    "        mask = mask.view(batch_size, -1, seq_len)\n",
    "        mask = mask.repeat(1, self.args.n_heads, 1)\n",
    "        mask = mask.view(batch_size*self.args.n_heads, -1, seq_len)\n",
    "\n",
    "        return mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "    def forward(self, input):\n",
    "        test, question, tag, _, mask, interaction, _ = input\n",
    "        batch_size = interaction.size(0)\n",
    "        seq_len = interaction.size(1)\n",
    "\n",
    "        # 신나는 embedding\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_tag,], 2)\n",
    "\n",
    "        embed = self.comb_proj(embed)\n",
    "\n",
    "        ### Encoder\n",
    "        mask = self.mask_2d_to_3d(mask, batch_size, seq_len).to(self.device)\n",
    "        for encoder in self.encoders:\n",
    "            embed = encoder(embed, mask)\n",
    "\n",
    "        ###################### DNN #####################\n",
    "        out = embed.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    \"\"\"\n",
    "    Load model and move tensors to a given devices.\n",
    "    \"\"\"\n",
    "    if args.model == 'bert': model = Bert(args)\n",
    "    if args.model == 'tfixup_bert': model = TfixupBert(args)\n",
    "    if args.model == 'tfixup': model = FixupEncoder(args)\n",
    "\n",
    "    model.to(args.device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model = 'tfixup'\n",
    "args.n_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Fixup 모델 사용 (1 Layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ☘️ Vanilla Encoder\n",
    "> 우리가 기본 세팅으로 사용하는 Transformer 인코더다. 결과 비교를 위해 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.6966268420219421\n",
      "Training steps: 50 Loss: 0.6849297285079956\n",
      "TRAIN AUC : 0.5990284499962166 ACC : 0.574018771331058\n",
      "VALID AUC : 0.6465815894892294 ACC : 0.6213930348258706\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.6233084201812744\n",
      "Training steps: 50 Loss: 0.6410101652145386\n",
      "TRAIN AUC : 0.6582421299162097 ACC : 0.6224402730375427\n",
      "VALID AUC : 0.6625833985446228 ACC : 0.6288557213930348\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.6255315542221069\n",
      "Training steps: 50 Loss: 0.6491870284080505\n",
      "TRAIN AUC : 0.6742461302553452 ACC : 0.6294795221843004\n",
      "VALID AUC : 0.6723725305253302 ACC : 0.6388059701492538\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.6515625715255737\n",
      "Training steps: 50 Loss: 0.6601109504699707\n",
      "TRAIN AUC : 0.6851740393670567 ACC : 0.6427047781569966\n",
      "VALID AUC : 0.6787359622161325 ACC : 0.6442786069651741\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.5685082674026489\n",
      "Training steps: 50 Loss: 0.7275938391685486\n",
      "TRAIN AUC : 0.693907778872776 ACC : 0.6518771331058021\n",
      "VALID AUC : 0.6836682168605153 ACC : 0.6457711442786069\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.654953122138977\n",
      "Training steps: 50 Loss: 0.6487596035003662\n",
      "TRAIN AUC : 0.7004117942542075 ACC : 0.6505972696245734\n",
      "VALID AUC : 0.6878635591379614 ACC : 0.6462686567164179\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.5821081399917603\n",
      "Training steps: 50 Loss: 0.6328425407409668\n",
      "TRAIN AUC : 0.7096184088532574 ACC : 0.6550767918088737\n",
      "VALID AUC : 0.6926589444598175 ACC : 0.6512437810945274\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.6005853414535522\n",
      "Training steps: 50 Loss: 0.637726902961731\n",
      "TRAIN AUC : 0.7162957297200218 ACC : 0.6574232081911263\n",
      "VALID AUC : 0.698323152437712 ACC : 0.6537313432835821\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.5559111833572388\n",
      "Training steps: 50 Loss: 0.5620694160461426\n",
      "TRAIN AUC : 0.7256948857104567 ACC : 0.6670221843003413\n",
      "VALID AUC : 0.7053258035369809 ACC : 0.6621890547263681\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.6076755523681641\n",
      "Training steps: 50 Loss: 0.5879924297332764\n",
      "TRAIN AUC : 0.7347390435105254 ACC : 0.672141638225256\n",
      "VALID AUC : 0.7106810637325058 ACC : 0.6611940298507463\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.5965550541877747\n",
      "Training steps: 50 Loss: 0.6121405959129333\n",
      "TRAIN AUC : 0.7414229282883599 ACC : 0.6768344709897611\n",
      "VALID AUC : 0.7143328959465853 ACC : 0.6656716417910448\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.6858922243118286\n",
      "Training steps: 50 Loss: 0.5976973176002502\n",
      "TRAIN AUC : 0.7459471951587509 ACC : 0.6823805460750854\n",
      "VALID AUC : 0.7156658841311921 ACC : 0.6661691542288557\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.5835003852844238\n",
      "Training steps: 50 Loss: 0.6224513053894043\n",
      "TRAIN AUC : 0.7509700457741633 ACC : 0.6821672354948806\n",
      "VALID AUC : 0.7177169403557214 ACC : 0.6646766169154229\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 14\n",
      "Training steps: 0 Loss: 0.5635050535202026\n",
      "Training steps: 50 Loss: 0.596644401550293\n",
      "TRAIN AUC : 0.7538589871338227 ACC : 0.6857935153583617\n",
      "VALID AUC : 0.7183878975781073 ACC : 0.6666666666666666\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 15\n",
      "Training steps: 0 Loss: 0.6348000764846802\n",
      "Training steps: 50 Loss: 0.4735870659351349\n",
      "TRAIN AUC : 0.7567614209773481 ACC : 0.6875\n",
      "VALID AUC : 0.719628151837669 ACC : 0.6661691542288557\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 16\n",
      "Training steps: 0 Loss: 0.5165059566497803\n",
      "Training steps: 50 Loss: 0.5441395044326782\n",
      "TRAIN AUC : 0.7608998757415169 ACC : 0.6900597269624573\n",
      "VALID AUC : 0.7209224595615619 ACC : 0.6686567164179105\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 17\n",
      "Training steps: 0 Loss: 0.626427948474884\n",
      "Training steps: 50 Loss: 0.563602864742279\n",
      "TRAIN AUC : 0.7640118989121228 ACC : 0.6915529010238908\n",
      "VALID AUC : 0.7202495187258061 ACC : 0.6587064676616915\n",
      "\n",
      "Start Training: Epoch 18\n",
      "Training steps: 0 Loss: 0.5341747403144836\n",
      "Training steps: 50 Loss: 0.5420560836791992\n",
      "TRAIN AUC : 0.7664659811633986 ACC : 0.6915529010238908\n",
      "VALID AUC : 0.7219405491236892 ACC : 0.6587064676616915\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 19\n",
      "Training steps: 0 Loss: 0.6026097536087036\n",
      "Training steps: 50 Loss: 0.5646877884864807\n",
      "TRAIN AUC : 0.7702061341572257 ACC : 0.6949658703071673\n",
      "VALID AUC : 0.7231693976063738 ACC : 0.66318407960199\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 20\n",
      "Training steps: 0 Loss: 0.5355725884437561\n",
      "Training steps: 50 Loss: 0.5540406703948975\n",
      "TRAIN AUC : 0.7725137135600374 ACC : 0.6979522184300341\n",
      "VALID AUC : 0.723723817543275 ACC : 0.6676616915422886\n",
      "\n",
      "saving model ...\n"
     ]
    }
   ],
   "source": [
    "args.Tfixup = False\n",
    "args.layer_norm = True\n",
    "\n",
    "setSeeds(42)\n",
    "report = run(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ☘️ Vanilla Encoder without LayerNorm\n",
    "> T-Fixup의 장점 중 하나는 가중치를 조절하여 학습이 원활하게 되어 Layer Norm의 도움이 필요없다는 것이다. 과연 T-Fixup 없이 Vanilla Transformer Encoder에서는 Layer Norm이 없다면 어떤 현상이 일어나는지 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "v9qV6aXonz1E",
    "outputId": "0d36ac2e-7ca2-4fc0-cf4c-ea296bc40ce4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.6926538944244385\n",
      "Training steps: 50 Loss: 0.6850431561470032\n",
      "TRAIN AUC : 0.5822891092863844 ACC : 0.5601535836177475\n",
      "VALID AUC : 0.6392561053140011 ACC : 0.6154228855721393\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.6339619159698486\n",
      "Training steps: 50 Loss: 0.6439831256866455\n",
      "TRAIN AUC : 0.6534924109336526 ACC : 0.6243600682593856\n",
      "VALID AUC : 0.6573228558875134 ACC : 0.6373134328358209\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.633604884147644\n",
      "Training steps: 50 Loss: 0.6509594917297363\n",
      "TRAIN AUC : 0.6642997081794537 ACC : 0.6350255972696246\n",
      "VALID AUC : 0.6637155458755223 ACC : 0.6442786069651741\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.649529218673706\n",
      "Training steps: 50 Loss: 0.6621315479278564\n",
      "TRAIN AUC : 0.6721720255226743 ACC : 0.6397184300341296\n",
      "VALID AUC : 0.6687980592326788 ACC : 0.6417910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.5865362882614136\n",
      "Training steps: 50 Loss: 0.7440720796585083\n",
      "TRAIN AUC : 0.6776995679670199 ACC : 0.6409982935153583\n",
      "VALID AUC : 0.6712622029415002 ACC : 0.6432835820895523\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.6704690456390381\n",
      "Training steps: 50 Loss: 0.6734933257102966\n",
      "TRAIN AUC : 0.6805202264184658 ACC : 0.6416382252559727\n",
      "VALID AUC : 0.6732185416276142 ACC : 0.6442786069651741\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.5944992899894714\n",
      "Training steps: 50 Loss: 0.6476452350616455\n",
      "TRAIN AUC : 0.683679837944329 ACC : 0.6424914675767918\n",
      "VALID AUC : 0.6746566613208287 ACC : 0.6442786069651741\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.614904522895813\n",
      "Training steps: 50 Loss: 0.6542548537254333\n",
      "TRAIN AUC : 0.6853578288770151 ACC : 0.6416382252559727\n",
      "VALID AUC : 0.6760382480329994 ACC : 0.6472636815920398\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.5627567768096924\n",
      "Training steps: 50 Loss: 0.605545163154602\n",
      "TRAIN AUC : 0.6869068207240906 ACC : 0.6424914675767918\n",
      "VALID AUC : 0.6773613181507566 ACC : 0.6482587064676617\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.6459220051765442\n",
      "Training steps: 50 Loss: 0.615356981754303\n",
      "TRAIN AUC : 0.6878212100023064 ACC : 0.6431313993174061\n",
      "VALID AUC : 0.6773236294967275 ACC : 0.6487562189054726\n",
      "\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.6446350812911987\n",
      "Training steps: 50 Loss: 0.6573388576507568\n",
      "TRAIN AUC : 0.6887312233398092 ACC : 0.6429180887372014\n",
      "VALID AUC : 0.6784766047680115 ACC : 0.6517412935323383\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.7368559837341309\n",
      "Training steps: 50 Loss: 0.6345621943473816\n",
      "TRAIN AUC : 0.6896470712649295 ACC : 0.643344709897611\n",
      "VALID AUC : 0.6784686703145316 ACC : 0.6512437810945274\n",
      "\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.6569337248802185\n",
      "Training steps: 50 Loss: 0.6789815425872803\n",
      "TRAIN AUC : 0.6907662181023552 ACC : 0.6439846416382252\n",
      "VALID AUC : 0.6792412877221276 ACC : 0.6492537313432836\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 14\n",
      "Training steps: 0 Loss: 0.6060395240783691\n",
      "Training steps: 50 Loss: 0.6326384544372559\n",
      "TRAIN AUC : 0.6906753261671226 ACC : 0.6450511945392492\n",
      "VALID AUC : 0.6790652420355444 ACC : 0.6492537313432836\n",
      "\n",
      "Start Training: Epoch 15\n",
      "Training steps: 0 Loss: 0.677696704864502\n",
      "Training steps: 50 Loss: 0.5506974458694458\n",
      "TRAIN AUC : 0.691244836617872 ACC : 0.64419795221843\n",
      "VALID AUC : 0.6797352074512453 ACC : 0.6492537313432836\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 16\n",
      "Training steps: 0 Loss: 0.559533953666687\n",
      "Training steps: 50 Loss: 0.6273397207260132\n",
      "TRAIN AUC : 0.6915873451441189 ACC : 0.6448378839590444\n",
      "VALID AUC : 0.6799608434720772 ACC : 0.6492537313432836\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 17\n",
      "Training steps: 0 Loss: 0.6523678302764893\n",
      "Training steps: 50 Loss: 0.5976018905639648\n",
      "TRAIN AUC : 0.691926936376557 ACC : 0.6435580204778157\n",
      "VALID AUC : 0.6797391746779852 ACC : 0.6492537313432836\n",
      "\n",
      "Start Training: Epoch 18\n",
      "Training steps: 0 Loss: 0.6335198283195496\n",
      "Training steps: 50 Loss: 0.5738982558250427\n",
      "TRAIN AUC : 0.69190979727543 ACC : 0.64419795221843\n",
      "VALID AUC : 0.68034814398256 ACC : 0.6507462686567164\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 19\n",
      "Training steps: 0 Loss: 0.707180380821228\n",
      "Training steps: 50 Loss: 0.6230074167251587\n",
      "TRAIN AUC : 0.6920617700497853 ACC : 0.6465443686006825\n",
      "VALID AUC : 0.6804066605769736 ACC : 0.6502487562189054\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 20\n",
      "Training steps: 0 Loss: 0.6547603607177734\n",
      "Training steps: 50 Loss: 0.6065438985824585\n",
      "TRAIN AUC : 0.6920033330081766 ACC : 0.6452645051194539\n",
      "VALID AUC : 0.6805787390368169 ACC : 0.6492537313432836\n",
      "\n",
      "saving model ...\n"
     ]
    }
   ],
   "source": [
    "args.Tfixup = False\n",
    "args.layer_norm = False\n",
    "\n",
    "setSeeds(42)\n",
    "report = run(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ☘️ T-Fixup Encoder\n",
    "> T-Fixup을 적용할 경우 Layer Norm없이 사용할 수 있다고 하지만, 과연 Layer Norm이 있다면 어떤 현상이 일어날까? 직접 알아보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Fixup Initialization Done\n",
      "T-Fixup Scaling Done\n",
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.9537209272384644\n",
      "Training steps: 50 Loss: 0.7271689176559448\n",
      "TRAIN AUC : 0.631950748787272 ACC : 0.6043088737201365\n",
      "VALID AUC : 0.6587619673874126 ACC : 0.6094527363184079\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.589790940284729\n",
      "Training steps: 50 Loss: 0.5833766460418701\n",
      "TRAIN AUC : 0.6937965570463129 ACC : 0.6448378839590444\n",
      "VALID AUC : 0.6874827053709307 ACC : 0.6338308457711442\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.6588139533996582\n",
      "Training steps: 50 Loss: 0.6599853038787842\n",
      "TRAIN AUC : 0.72067714035921 ACC : 0.6702218430034129\n",
      "VALID AUC : 0.7025541997558172 ACC : 0.654228855721393\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.5958243608474731\n",
      "Training steps: 50 Loss: 0.6717612147331238\n",
      "TRAIN AUC : 0.7388183319096331 ACC : 0.6832337883959044\n",
      "VALID AUC : 0.7114130170660176 ACC : 0.6577114427860696\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.5956878662109375\n",
      "Training steps: 50 Loss: 0.5725131630897522\n",
      "TRAIN AUC : 0.7509911049888459 ACC : 0.6902730375426621\n",
      "VALID AUC : 0.7156787776180968 ACC : 0.6611940298507463\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.5857251882553101\n",
      "Training steps: 50 Loss: 0.5054757595062256\n",
      "TRAIN AUC : 0.7605962948545317 ACC : 0.7005119453924915\n",
      "VALID AUC : 0.7199911530843699 ACC : 0.6671641791044776\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.5452830195426941\n",
      "Training steps: 50 Loss: 0.5559461712837219\n",
      "TRAIN AUC : 0.7718729117418341 ACC : 0.7109641638225256\n",
      "VALID AUC : 0.7223466939611867 ACC : 0.6656716417910448\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.5729340314865112\n",
      "Training steps: 50 Loss: 0.6517788171768188\n",
      "TRAIN AUC : 0.7800470778288405 ACC : 0.710537542662116\n",
      "VALID AUC : 0.7249382848290273 ACC : 0.664179104477612\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.6115782260894775\n",
      "Training steps: 50 Loss: 0.5022149085998535\n",
      "TRAIN AUC : 0.7894688428462943 ACC : 0.722056313993174\n",
      "VALID AUC : 0.7277927044683867 ACC : 0.6597014925373135\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.5794670581817627\n",
      "Training steps: 50 Loss: 0.5878362655639648\n",
      "TRAIN AUC : 0.7994511841022074 ACC : 0.7280290102389079\n",
      "VALID AUC : 0.730604476420292 ACC : 0.6666666666666666\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.5436205863952637\n",
      "Training steps: 50 Loss: 0.4536454677581787\n",
      "TRAIN AUC : 0.808997845760853 ACC : 0.735494880546075\n",
      "VALID AUC : 0.7347760153372985 ACC : 0.6746268656716418\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.5095828175544739\n",
      "Training steps: 50 Loss: 0.5214478969573975\n",
      "TRAIN AUC : 0.8183190553073323 ACC : 0.7414675767918089\n",
      "VALID AUC : 0.734882138652591 ACC : 0.6766169154228856\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.4821051359176636\n",
      "Training steps: 50 Loss: 0.5929241180419922\n",
      "TRAIN AUC : 0.8256353546836697 ACC : 0.746160409556314\n",
      "VALID AUC : 0.7304983531049996 ACC : 0.66318407960199\n",
      "\n",
      "Start Training: Epoch 14\n",
      "Training steps: 0 Loss: 0.49624940752983093\n",
      "Training steps: 50 Loss: 0.42076531052589417\n",
      "TRAIN AUC : 0.8331656193278009 ACC : 0.7536262798634812\n",
      "VALID AUC : 0.7369039365799134 ACC : 0.6686567164179105\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 15\n",
      "Training steps: 0 Loss: 0.599375307559967\n",
      "Training steps: 50 Loss: 0.6179776191711426\n",
      "TRAIN AUC : 0.841799350355135 ACC : 0.7617320819112628\n",
      "VALID AUC : 0.7360306507937926 ACC : 0.673134328358209\n",
      "\n",
      "Start Training: Epoch 16\n",
      "Training steps: 0 Loss: 0.4546038508415222\n",
      "Training steps: 50 Loss: 0.44854146242141724\n",
      "TRAIN AUC : 0.8470909566626887 ACC : 0.7659982935153583\n",
      "VALID AUC : 0.7379993870634687 ACC : 0.6771144278606965\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 17\n",
      "Training steps: 0 Loss: 0.5463418960571289\n",
      "Training steps: 50 Loss: 0.4420551359653473\n",
      "TRAIN AUC : 0.8550373003363094 ACC : 0.76919795221843\n",
      "VALID AUC : 0.7298219409458463 ACC : 0.6696517412935323\n",
      "\n",
      "Start Training: Epoch 18\n",
      "Training steps: 0 Loss: 0.560177743434906\n",
      "Training steps: 50 Loss: 0.40747565031051636\n",
      "TRAIN AUC : 0.8617740610644294 ACC : 0.7755972696245734\n",
      "VALID AUC : 0.7393651048686798 ACC : 0.681592039800995\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 19\n",
      "Training steps: 0 Loss: 0.4843994379043579\n",
      "Training steps: 50 Loss: 0.5721266865730286\n",
      "TRAIN AUC : 0.8677676411947413 ACC : 0.7807167235494881\n",
      "VALID AUC : 0.7312575811223483 ACC : 0.6626865671641791\n",
      "\n",
      "Start Training: Epoch 20\n",
      "Training steps: 0 Loss: 0.3786582946777344\n",
      "Training steps: 50 Loss: 0.3108537793159485\n",
      "TRAIN AUC : 0.8707875872795048 ACC : 0.7811433447098977\n",
      "VALID AUC : 0.7379135957852183 ACC : 0.6721393034825871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args.Tfixup = True\n",
    "args.layer_norm = True\n",
    "\n",
    "setSeeds(42)\n",
    "report = run(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ☘️ T-Fixup Encoder without LayerNorm\n",
    "> 우리가 보고자 하던 진짜 T-Fixup 세팅이다. 어떤 효과가 있는지 확인해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Fixup Initialization Done\n",
      "T-Fixup Scaling Done\n",
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.7209408283233643\n",
      "Training steps: 50 Loss: 0.6976499557495117\n",
      "TRAIN AUC : 0.5798749574941175 ACC : 0.552901023890785\n",
      "VALID AUC : 0.6350072054755663 ACC : 0.6034825870646766\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.6573376059532166\n",
      "Training steps: 50 Loss: 0.6300456523895264\n",
      "TRAIN AUC : 0.6458313238952801 ACC : 0.6316126279863481\n",
      "VALID AUC : 0.6596025235529293 ACC : 0.6457711442786069\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.641427755355835\n",
      "Training steps: 50 Loss: 0.6824603080749512\n",
      "TRAIN AUC : 0.6639887428925151 ACC : 0.6392918088737202\n",
      "VALID AUC : 0.6720432507059184 ACC : 0.6472636815920398\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.677515983581543\n",
      "Training steps: 50 Loss: 0.6937620639801025\n",
      "TRAIN AUC : 0.6810587406224957 ACC : 0.6405716723549488\n",
      "VALID AUC : 0.6829491570139081 ACC : 0.6482587064676617\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.6606433391571045\n",
      "Training steps: 50 Loss: 0.6594195365905762\n",
      "TRAIN AUC : 0.6953081710664624 ACC : 0.6407849829351536\n",
      "VALID AUC : 0.6928325106296881 ACC : 0.6512437810945274\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.6397457122802734\n",
      "Training steps: 50 Loss: 0.5850180983543396\n",
      "TRAIN AUC : 0.7069008585048687 ACC : 0.6427047781569966\n",
      "VALID AUC : 0.7001847735854111 ACC : 0.6537313432835821\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.6604081392288208\n",
      "Training steps: 50 Loss: 0.5717697143554688\n",
      "TRAIN AUC : 0.715086237939953 ACC : 0.6469709897610921\n",
      "VALID AUC : 0.7060309780899985 ACC : 0.6592039800995025\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.6538784503936768\n",
      "Training steps: 50 Loss: 0.7052169442176819\n",
      "TRAIN AUC : 0.7218343031815824 ACC : 0.6497440273037542\n",
      "VALID AUC : 0.7106572603720664 ACC : 0.66318407960199\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.6872067451477051\n",
      "Training steps: 50 Loss: 0.5624848008155823\n",
      "TRAIN AUC : 0.7240793430983666 ACC : 0.6574232081911263\n",
      "VALID AUC : 0.7141603215833996 ACC : 0.6661691542288557\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.6118553876876831\n",
      "Training steps: 50 Loss: 0.6591261625289917\n",
      "TRAIN AUC : 0.7288678073893229 ACC : 0.6631825938566553\n",
      "VALID AUC : 0.7171967377494519 ACC : 0.663681592039801\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.6176837086677551\n",
      "Training steps: 50 Loss: 0.559539794921875\n",
      "TRAIN AUC : 0.7312687401940182 ACC : 0.6651023890784983\n",
      "VALID AUC : 0.7201002518197175 ACC : 0.6666666666666666\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.5995242595672607\n",
      "Training steps: 50 Loss: 0.5981087684631348\n",
      "TRAIN AUC : 0.7343480349747062 ACC : 0.6702218430034129\n",
      "VALID AUC : 0.7224349647561495 ACC : 0.6666666666666666\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.6016544103622437\n",
      "Training steps: 50 Loss: 0.6562455892562866\n",
      "TRAIN AUC : 0.7378858918759749 ACC : 0.6700085324232082\n",
      "VALID AUC : 0.7244324634196899 ACC : 0.6681592039800995\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 14\n",
      "Training steps: 0 Loss: 0.5729255676269531\n",
      "Training steps: 50 Loss: 0.5841284990310669\n",
      "TRAIN AUC : 0.7414654113794515 ACC : 0.6783276450511946\n",
      "VALID AUC : 0.726330781414733 ACC : 0.663681592039801\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 15\n",
      "Training steps: 0 Loss: 0.6368453502655029\n",
      "Training steps: 50 Loss: 0.6417542695999146\n",
      "TRAIN AUC : 0.7451811320376294 ACC : 0.6813139931740614\n",
      "VALID AUC : 0.728663510737795 ACC : 0.6686567164179105\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 16\n",
      "Training steps: 0 Loss: 0.6033229827880859\n",
      "Training steps: 50 Loss: 0.5951812267303467\n",
      "TRAIN AUC : 0.749159226807742 ACC : 0.6845136518771331\n",
      "VALID AUC : 0.7303307377752388 ACC : 0.6701492537313433\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 17\n",
      "Training steps: 0 Loss: 0.6203032732009888\n",
      "Training steps: 50 Loss: 0.564262866973877\n",
      "TRAIN AUC : 0.7527106673583084 ACC : 0.6860068259385665\n",
      "VALID AUC : 0.7319652351920782 ACC : 0.6736318407960199\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 18\n",
      "Training steps: 0 Loss: 0.661207914352417\n",
      "Training steps: 50 Loss: 0.5660449862480164\n",
      "TRAIN AUC : 0.7572423186286531 ACC : 0.689419795221843\n",
      "VALID AUC : 0.7344591331014488 ACC : 0.672636815920398\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 19\n",
      "Training steps: 0 Loss: 0.603628396987915\n",
      "Training steps: 50 Loss: 0.669472336769104\n",
      "TRAIN AUC : 0.7621307913979946 ACC : 0.6911262798634812\n",
      "VALID AUC : 0.7366445791317923 ACC : 0.6746268656716418\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 20\n",
      "Training steps: 0 Loss: 0.541073203086853\n",
      "Training steps: 50 Loss: 0.509019672870636\n",
      "TRAIN AUC : 0.769714205488706 ACC : 0.6994453924914675\n",
      "VALID AUC : 0.7386014137212488 ACC : 0.6776119402985075\n",
      "\n",
      "saving model ...\n"
     ]
    }
   ],
   "source": [
    "args.Tfixup = True\n",
    "args.layer_norm = False\n",
    "\n",
    "setSeeds(42)\n",
    "report = run(args, train_data, valid_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3강_lstm_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
